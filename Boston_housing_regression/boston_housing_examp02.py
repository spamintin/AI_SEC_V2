# -*- coding: utf-8 -*-
"""boston_housing_examp02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x48gaom1ehgy255b6noCTr7dzibBgIhQ
"""

#1. Data Collection
import tensorflow as tf
(x_train, y_train), (x_test, y_test)=tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=113
)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#2. Data Analysis
#  CRIM     per capita crime rate by town
#  ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
#  INDUS    proportion of non-retail business acres per town
#  CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
#  NOX      nitric oxides concentration (parts per 10 million)
#  RM       average number of rooms per dwelling
#  AGE      proportion of owner-occupied units built prior to 1940
#  DIS      weighted distances to five Boston employment centres
#  RAD      index of accessibility to radial highways
#  TAX      full-value property-tax rate per $10,000
#  PTRATIO  pupil-teacher ratio by town
#  B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
#  LSTAT    % lower status of the population
#  MEDV     Median value of owner-occupied homes in $1000's

#data_label
for ix,d in enumerate(x_train[0]):
  print(ix+1,"type:",type(d))
print("target_data_type:",type(y_train[0]))
data_label= ["CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO","B","LSTAT"]
for ix,d in enumerate(x_train[0]):
  print(f"{ix}. {data_label[ix]}: {d}", end="|")
  print("MEDV:",y_train[0]*1000*1400,"ì›")

import matplotlib.pyplot as plt
for ix in range(len(x_train[0])):
  plt.subplot(4,4,ix+1)
  plt.scatter(y_train,x_train[:,ix],label=data_label[ix],s=1)
  plt.title(data_label[ix])
plt.show()

# Dataset size
for ix in range(len(x_train[0])):
  print(ix,".")
  print("max",x_train[:,ix].max(),end=":")
  print("min",x_train[:,ix].min(),end=":")
  print("std",x_train[:,ix].std(),end=":")
  print("mean",x_train[:,ix].mean(),end=":")
  print()

# Data preprocessing - Normalize and scale the data excluding columns 3 (CHAS) and 4 (NOX)
except_datas=[3,4]
norm_mean=[]
norm_std=[]
for ix in range(len(x_train[0])):
  if ix in except_datas:
    norm_mean.append(0)
    norm_std.append(0)
    continue
  norm_mean.append(x_train[:,ix].mean())
  norm_std.append(x_train[:,ix].std())
def normal_data(target_data):
  for ix in range(len(target_data[0])):
    if ix in except_datas:
      continue
      #Scale the data to a standard normal distribution.
    target_data[:,ix]=(target_data[:,ix]-norm_mean[ix])/norm_std[ix]
  return target_data

x_train= normal_data(x_train)
x_test= normal_data(x_test)
print(x_train[:3])
print(x_test[:3])
print(x_train[:,0].std())
print(x_train[:,0].mean())
print(x_train[:,-1].std())
print(x_train[:,-1].mean())
print(x_train[:].max())
print(x_test[:].max())

#4. Model Creation
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import Dense, Dropout
print(x_train.shape)

import numpy as np
import random
random.seed(123)
np.random.seed(123)
tf.random.set_seed(123)
model= Sequential()
model.add(Input((13,)))
model.add(Dense(256,activation="relu"))
# res=model(np.array([x_train[0]]))
# print(len(res[0]))
# print(res)
model.add(Dropout(0.4))
model.add(Dense(64,activation="relu"))
model.add(Dropout(0.3))
model.add(Dense(16,activation="relu"))
model.add(Dense(1))
model.compile(loss="MSE",optimizer="SGD",metrics=["MAE"])

print(x_train[:].max())
print(x_test[:].max())
print(x_train[:].min())
print(x_test[:].min())

#Begin Training
y_mean=y_train.mean()
y_std=y_train.std()
y_train=(y_train-y_mean)/y_std
y_test=(y_test-y_mean)/y_std
fhist= model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=500)

plt.plot(fhist.history["loss"],label="mse")
plt.plot(fhist.history["MAE"],label="mae")
plt.legend()
plt.show()

y_pred=model.predict(x_test)
plt.plot(y_test,y_test,label="y_True")
plt.scatter(y_pred,y_pred,label="y_Pred",c="red",s=1)
plt.legend()
plt.show()
print(y_test[0])
print(y_pred[0])

#Error Rate Calculation
print(y_pred.shape)
print(y_test.shape)
y_pred=y_pred.reshape(-1)
print(y_pred.shape)

#t= (y-mean)/std> y=t*std+mean
#y_mean
#y_std
y_pred=y_pred*y_std+y_mean
y_test=y_test*y_std+y_mean
print(y_pred[1])
print(y_test[1])
print((1-(y_pred[1]/y_test[1]))*100)
rate=1-y_pred/y_test
print(rate[:5])
rate= np.absolute(rate)
err_rate_mean= rate[:5].mean()
print(f"The current modelâ€™s overall mean error percentage is {err_rate_mean:.2%}")

